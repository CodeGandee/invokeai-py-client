<?xml version="1.0" encoding="UTF-8"?>
<memory>
  <item id="mem_1754764473772_83tcz0ewn" time="2025/08/10 02:34">
    <content>
      I have a set of powerful CLI tools for web API analysis and automation. These include: curl for making HTTP requests, jq for JSON processing, httpie as a user-friendly curl alternative, Node.js for running JavaScript tools, Playwright and Puppeteer for browser automation, mitmproxy for inspecting HTTPS traffic, shot-scraper for taking website screenshots, and yq for processing YAML files. The documentation for these tools is available in &#x27;context/hints/howto-use-webapi-tools.md&#x27;.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754764750429_mrjec5nih" time="2025/08/10 02:39">
    <content>
      To get JS-rendered pages, I will use the `shot-scraper` tool, specifically the `html` or `javascript` commands. I must remember to wait for at least 5 seconds (`--wait 5000`) to ensure the page has fully loaded all dynamic content. I should also be aware of Content Security Policy (CSP) and use the `--bypass-csp` flag if needed. The full instructions are in `context/hints/howto-use-shot-scraper-for-js-rendered-content.md`.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772295825_1o1ha1q8n" time="2025/08/10 04:44">
    <content>
      InvokeAI Boards API solution: To get board names, use GET /api/v1/boards/?all=true which returns a JSON array of board objects. Each board has a &#x27;board_name&#x27; field that contains the name. The API supports pagination with offset/limit, but using all=true gets everything at once. Other useful fields include board_id, image_count, created_at, and cover_image_name. The API is running on localhost:9090 in this project.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772483945_pkk212495" time="2025/08/10 04:48">
    <content>
      InvokeAI API exploration project rules: 1) DO NOT create scripts in workspace root, 2) DO NOT modify task files, 3) If API exploration is successful, create demos in context/examples/api-demo-&lt;what-api&gt;.py. Successfully completed Task 1 (get board names) using GET /api/v1/boards/?all=true and created proper demo at context/examples/api-demo-boards.py showing how to extract board_name fields from the JSON response.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772655572_gyrzcg18r" time="2025/08/10 04:50">
    <content>
      Enhanced InvokeAI boards API with uncategorized images: Use GET /api/v1/boards/none/image_names to get uncategorized images (images not assigned to any board). The &#x27;none&#x27; keyword is a special board_id for uncategorized content. Updated api-demo-boards.py to include comprehensive overview showing total image counts across boards (73 images) and uncategorized (470 images) for total of 543 images. This provides complete visibility into all images in the InvokeAI system.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772751952_ci3sg5btx" time="2025/08/10 04:52">
    <content>
      Moved InvokeAI boards API demo to correct location: examples/api-demo-boards.py (moved from context/examples/). The demo works perfectly from the new location and includes full boards functionality with uncategorized images support. File organization follows project structure with API demos in the root examples/ directory rather than context/examples/.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754923706414_a8slzad1r" time="2025/08/11 22:48">
    <content>
      InvokeAI workflow schema discovery methods for Python client development:
    
      1. **OpenAPI Schema Endpoint**: curl http://localhost:9090/openapi.json provides complete node definitions, field constraints, validation rules, and default values
    
      2. **Key jq Commands for Schema Analysis**:
      - Find node schema: `jq --arg type &quot;NODE_TYPE&quot; &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == $type)) | .[0].value&#x27; openapi.json`
      - List input fields: `jq --arg type &quot;NODE_TYPE&quot; &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == $type)) | .[0].value.properties | to_entries[] | select(.value.field_kind == &quot;input&quot;) | .key&#x27; openapi.json`
      - Extract required fields: `jq &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == &quot;NODE_TYPE&quot;)) | .[0].value.properties | to_entries[] | select(.value.orig_required == true) | .key&#x27; openapi.json`
    
      3. **Common Field Types for Client Models**:
      - ImageField: `{&quot;image_name&quot;: &quot;uuid.png&quot;}`
      - LatentsField: `{&quot;latents_name&quot;: &quot;uuid&quot;}`
      - ModelField: `{&quot;key&quot;: &quot;model-id&quot;, &quot;hash&quot;: &quot;...&quot;, &quot;name&quot;: &quot;model-name&quot;, &quot;base&quot;: &quot;sdxl&quot;, &quot;type&quot;: &quot;main&quot;}`
      - Basic types: StringField, IntegerField, FloatField, BooleanField with validation constraints (ge, le, gt, lt)
    
      4. **Workflow Structure (v3.0.0)**: Essential components are nodes[] array with {id, type, data: {type, inputs, version, nodePack}}, edges[] array connecting nodes via sourceHandle/targetHandle, and meta.version for schema compatibility
    
      5. **Source Discovery**: Use `grep -r &quot;@invocation.*\&quot;NODE_TYPE\&quot;&quot; invokeai/app/invocations/` to find Python implementation files for understanding node behavior
    
      Reference: context/hints/invokeai-kb/howto-find-invokeai-workflow-schema.md
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754925791966_k476kfg9q" time="2025/08/11 23:23">
    <content>
      Created comprehensive InvokeAI workflow input types documentation with 60+ field types identified from source code analysis:
    
      **Core Documentation**: `about-invokeai-workflow-input-types.md` covers:
      - 9 primitive types (int, float, bool, str, ColorField, BoundingBoxField, etc.)
      - 8 resource reference types (ImageField, LatentsField, TensorField, etc.)
      - 8 conditioning types for different model architectures (FLUX, SD3, CogView4)
      - 12+ model reference types (ModelIdentifierField, UNetField, CLIPField, etc.)
      - Collection types (list[T] for all supported types)
      - Enum/Literal types and UI configuration fields
      - Advanced features: connection types, validation constraints, mixins
    
      **Implementation Guide**: `howto-implement-invokeai-workflow-fields.md` provides:
      - Pydantic model patterns for client-side field validation
      - Node input builders with fluent API
      - Type-safe field validators and OpenAPI schema integration
      - Complete workflow construction examples (txt2img, img2img+ControlNet)
      - Connection graph utilities and dependency resolution
      - Error handling patterns and comprehensive test examples
    
      **Key Source Files Analyzed**:
      - `fields.py` - Field type definitions
      - `baseinvocation.py` - Base classes and decorators
      - `primitives.py` - Basic input/output types
      - `model.py` - Model reference types
      - 50+ invocation files - Usage patterns and validation rules
    
      Reference location: context/hints/invokeai-kb/
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755286376277_wft1tdu4p" time="2025/08/16 03:32">
    <content>
      CRITICAL: InvokeAI workflow form fields have total ordering with unique input-index
    
      The input-index is a 0-based index assigned to each node-field element based on depth-first tree traversal of the form structure. This index uniquely identifies each input field.
    
      Example from sdxl-flux-refine workflow (24 total input fields):
      - [0] Positive: Positive Prompt
      - [1] Negative: Negative Prompt
      - [2] integer: Output Width
      - [3] integer: Output Height
      - [4] sdxl_model_loader: SDXL Model
      ...
      - [23] float_math: Noise Ratio
    
      Key implementation details:
      1. Only node-field elements get an input-index (not containers, dividers, or text)
      2. Index is determined by tree traversal order starting from root
      3. Container children are processed in their listed order
      4. The input-index is essential for API calls where inputs may be provided as ordered arrays
    
      Python code to extract indices:
      ```python
      def traverse_form(elem_id, form_elements, input_fields):
      elem = form_elements.get(elem_id)
      if elem.get(&#x27;type&#x27;) == &#x27;container&#x27;:
      for child_id in elem[&#x27;data&#x27;].get(&#x27;children&#x27;, []):
      traverse_form(child_id, form_elements, input_fields)
      elif elem.get(&#x27;type&#x27;) == &#x27;node-field&#x27;:
      input_fields.append({&#x27;input_index&#x27;: len(input_fields), ...})
      ```
    
      Updated documentation at: context/hints/invokeai-kb/howto-interpret-workflow-form-field.md
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755287246642_qc2hbthqh" time="2025/08/16 03:47">
    <content>
      InvokeAI Model Resolution for Workflows - CRITICAL
    
      Model references in workflows have deployment-specific keys and hashes. Only the model NAME is consistent across deployments.
    
      API Endpoint: GET /api/v2/models/get_by_attrs
      Parameters: name, type, base
    
      Example workflow model field:
      ```json
      {
      &quot;key&quot;: &quot;cc644930-283a-464f-b48f-831e6add8ed5&quot;,  // DEPLOYMENT-SPECIFIC
      &quot;hash&quot;: &quot;blake3:38aafa...&quot;,                     // MAY DIFFER
      &quot;name&quot;: &quot;t5_bnb_int8_quantized_encoder&quot;,        // CONSISTENT
      &quot;base&quot;: &quot;any&quot;,
      &quot;type&quot;: &quot;t5_encoder&quot;
      }
      ```
    
      Resolution process:
      1. Extract model name, type, base from workflow
      2. Query local InvokeAI instance: /api/v2/models/get_by_attrs
      3. Update workflow with local key and hash
      4. Submit workflow with correct local references
    
      Python implementation:
      ```python
      def find_model_by_name(base_url, model_name, model_type, model_base):
      response = requests.get(f&quot;{base_url}/api/v2/models/get_by_attrs&quot;,
      params={&quot;name&quot;: model_name, &quot;type&quot;: model_type, &quot;base&quot;: model_base})
      return response.json() if response.status_code == 200 else None
      ```
    
      Common model types:
      - t5_encoder: T5 text encoder for FLUX
      - main: Primary diffusion models (flux, sdxl)
      - vae: VAE models
      - clip: CLIP text encoders
    
      Task 1.2 completed in: context/tasks/features/task-explore-workflow.md
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755657923728_nut60czn8" time="2025/08/20 10:45">
    <content>
      InvokeAI Python Client Project Architecture and Design (2025-08-20)
    
      ## Project Overview
      Python client for InvokeAI server providing typed wrapper around REST + Socket.IO APIs. Three main domains: Workflows (load/inspect/submit/monitor), Boards &amp; Images (CRUD operations), DNN Models (v2 API discovery).
    
      ## Core Design Principles (from context/design/usage-pattern.md)
      1. **Workflow JSON Preservation**: Original GUI workflow JSON (`WorkflowDefinition.raw_data`) is NEVER modified directly. Only deep copies are mutated during submission.
      2. **JSONPath-based Updates**: Each input field has a JSONPath pointing to dict in workflow JSON. Updates via `IvkWorkflowInput.field.to_api_format()` only modify VALUES, never add/delete keys.
      3. **Input Discovery**: Inputs discovered from `form.elements` tree (depth-first from root), NOT from exposedFields.
      4. **Edge-Connected Inputs**: Retained by default in submission payload to match GUI behavior (env var INVOKEAI_PRUNE_CONNECTED_FIELDS=1 to prune).
    
      ## Key Components (from README.md)
      - `InvokeAIClient`: Connection façade with retrying HTTP + async Socket.IO
      - `WorkflowDefinition`: Preserved raw JSON + metadata
      - `WorkflowHandle`: Mutable execution state, submission logic, JSONPath updates
      - `IvkWorkflowInput`: Public parameters from form tree
      - `IvkField` types: Default-constructable typed wrappers (primitives, resources, models, complex)
    
      ## Workflow Submission Process (README.md lines 356-374)
      1. Input Discovery: Traverse form.elements depth-first, build IvkWorkflowInput with JSONPath to `...nodes[?(@.id=&#x27;{node_id}&#x27;)].data.inputs.{field}.value`
      2. Value Mutation: Deep copy raw_data, write field values via JSONPath
      3. Node Extraction: Build minimal API nodes with id, type, is_intermediate, use_cache, inputs
      4. Edge Conversion: GUI edges → API format `{source: {node_id, field}, destination: {node_id, field}}`
      5. Board Injection: Apply board_id to output nodes (save_image, l2i, flux_vae_decode/encode, hed_edge_detection)
      6. Batch Envelope: POST to `/api/v1/queue/{queue_id}/enqueue_batch`
      7. Tracking: Get batch_id, item_ids, session_id for monitoring
    
      ## Repository Pattern Architecture
      - `BoardRepository` &amp; `BoardHandle`: Board/image management with uncategorized handling
      - `WorkflowRepository`: Workflow lifecycle management
      - `DnnModelRepository`: Model discovery via v2 endpoints (/../v2/models/ traversal)
    
      ## Submission Modes
      - Sync: `submit_sync()` + polling `wait_for_completion_sync()`
      - Async: `await submit(..., subscribe_events=True)` with callbacks
      - Streaming: `async for event in submit_sync_monitor_async()`
    
      ## Example Workflows (data/workflows/)
      - sdxl-text-to-image.json → call-wf-sdxl-text-to-image.json
      - sdxl-flux-refine.json → call-wf-sdxl-flux-refine.json
      - flux-image-to-image.json → call-wf-flux-image-to-image-1.json
    
      ## Recent Implementation (Aug 19 commit 53de014)
      Fixed workflow field update pattern to properly merge dicts preserving keys (name, label, description). JSONPath now points to entire field dict, not just .value. Added comprehensive tests for SDXL, FLUX, image-to-image workflows.
    
      ## Code Review Issues (context/logs/code-reivew/20250819-174650-src-review.md)
      1. JSONPath should point to ...inputs.{field}.value (currently points to field dict)
      2. IvkModelIdentifierField should return {&quot;value&quot;: {...}} not top-level keys
      3. IvkModelIdentifierField violates default-constructability requirement
      4. README vs code mismatch on pruning connected inputs default behavior
    
      References: context/design/usage-pattern.md, README.md, context/tasks/task-check-design.md
    </content>
    <tags>#其他</tags>
  </item>
</memory>