<?xml version="1.0" encoding="UTF-8"?>
<memory>
  <item id="mem_1754764473772_83tcz0ewn" time="2025/08/10 02:34">
    <content>
      I have a set of powerful CLI tools for web API analysis and automation. These include: curl for making HTTP requests, jq for JSON processing, httpie as a user-friendly curl alternative, Node.js for running JavaScript tools, Playwright and Puppeteer for browser automation, mitmproxy for inspecting HTTPS traffic, shot-scraper for taking website screenshots, and yq for processing YAML files. The documentation for these tools is available in &#x27;context/hints/howto-use-webapi-tools.md&#x27;.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754764750429_mrjec5nih" time="2025/08/10 02:39">
    <content>
      To get JS-rendered pages, I will use the `shot-scraper` tool, specifically the `html` or `javascript` commands. I must remember to wait for at least 5 seconds (`--wait 5000`) to ensure the page has fully loaded all dynamic content. I should also be aware of Content Security Policy (CSP) and use the `--bypass-csp` flag if needed. The full instructions are in `context/hints/howto-use-shot-scraper-for-js-rendered-content.md`.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772295825_1o1ha1q8n" time="2025/08/10 04:44">
    <content>
      InvokeAI Boards API solution: To get board names, use GET /api/v1/boards/?all=true which returns a JSON array of board objects. Each board has a &#x27;board_name&#x27; field that contains the name. The API supports pagination with offset/limit, but using all=true gets everything at once. Other useful fields include board_id, image_count, created_at, and cover_image_name. The API is running on localhost:9090 in this project.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772483945_pkk212495" time="2025/08/10 04:48">
    <content>
      InvokeAI API exploration project rules: 1) DO NOT create scripts in workspace root, 2) DO NOT modify task files, 3) If API exploration is successful, create demos in context/examples/api-demo-&lt;what-api&gt;.py. Successfully completed Task 1 (get board names) using GET /api/v1/boards/?all=true and created proper demo at context/examples/api-demo-boards.py showing how to extract board_name fields from the JSON response.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772655572_gyrzcg18r" time="2025/08/10 04:50">
    <content>
      Enhanced InvokeAI boards API with uncategorized images: Use GET /api/v1/boards/none/image_names to get uncategorized images (images not assigned to any board). The &#x27;none&#x27; keyword is a special board_id for uncategorized content. Updated api-demo-boards.py to include comprehensive overview showing total image counts across boards (73 images) and uncategorized (470 images) for total of 543 images. This provides complete visibility into all images in the InvokeAI system.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754772751952_ci3sg5btx" time="2025/08/10 04:52">
    <content>
      Moved InvokeAI boards API demo to correct location: examples/api-demo-boards.py (moved from context/examples/). The demo works perfectly from the new location and includes full boards functionality with uncategorized images support. File organization follows project structure with API demos in the root examples/ directory rather than context/examples/.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754773163373_efwsmgu59" time="2025/08/10 04:59">
    <content>
      Task 2 completed: InvokeAI latest image download from board. Key API endpoints: GET /api/v1/images/names?board_id={id}&amp;order_dir=DESC gets ordered image names (most recent first), GET /api/v1/images/i/{name}/metadata gets generation details, GET /api/v1/images/i/{name}/full downloads full resolution image. Successfully downloaded latest image from &#x27;probe&#x27; board (ad7ae269-135e-4432-ac9a-db349cae64d2.png, 1.83MB, SDXL txt2img) to ./tmp/downloads/. API response format: {image_names: [string], total_count: int, starred_count: int}.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754773327802_rdmwxhwmh" time="2025/08/10 05:02">
    <content>
      Corrected Task 2: InvokeAI latest image detection requires actual timestamp sorting, not board ordering. Critical fix: Use POST /api/v1/images/images_by_names with all board image names to get ImageDTOs containing created_at timestamps, then sort by created_at desc. Board image ordering via GET /api/v1/images/names is NOT chronological. True latest: d5ecea7e-a8ad-4a41-814c-0100c191adaa.png (2025-08-09 20:56:35.827) vs previous ad7ae269... which was NOT the latest by timestamp.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754773620373_05op8nsxc" time="2025/08/10 05:07">
    <content>
      InvokeAI starred images from board: Use GET /api/v1/boards/{board_id}/image_names to get all images, then POST /api/v1/images/images_by_names to get ImageDTOs, filter by starred=true field. Found 2 starred images in probe board: ad7ae269-135e-4432-ac9a-db349cae64d2.png (2025-08-09 19:40:52.348) and 0ab4c196-f00f-449b-8936-82a76c10167c.png (2025-08-09 15:45:57.219). Created api-demo-starred-images.py with download functionality and get-starred-from-probe.py for simple listing.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754913107721_3bf7tuh3t" time="2025/08/11 19:51">
    <content>
      Task 5 completed: InvokeAI image upload to board. Key API: POST /api/v1/images/upload with multipart/form-data. Required params: image_category=&#x27;user&#x27;, is_intermediate=false, board_id={target_board_id}. Form data includes &#x27;file&#x27; field with binary image data. Successfully uploaded data/images/ad7ae269-135e-4432-ac9a-db349cae64d2.png (1.83MB, 960x1280) to &#x27;samples&#x27; board (d32f49b7-370e-4bab-8cfd-cf81fcf4c3a8). Returns ImageDTO with new image_name. Created api-demo-upload-image.py with board creation, upload, and verification functionality.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754916383145_t0f3eq6p9" time="2025/08/11 20:46">
    <content>
      Created comprehensive InvokeAI workflow schema guide at context/hints/howto-use-invokeai-workflow-schema.md. Document covers complete workflow JSON structure (v3.0.0), node/edge schemas, field types (StringField, ImageField, etc.), core invocation categories (SDXL, FLUX, image processing, ControlNet), custom node development patterns, InvocationContext services, and common workflow patterns. Based on deep analysis of InvokeAI source code, TypeScript schemas, Python invocation classes, and official documentation. Guide includes practical examples for custom nodes, field validation, and troubleshooting.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754916512287_hx1hel8vq" time="2025/08/11 20:48">
    <content>
      Enhanced InvokeAI workflow schema guide with comprehensive &quot;Discovering Node Schemas&quot; section. Added 6 methods for finding node JSON schemas: 1) OpenAPI endpoint (http://localhost:9090/openapi.json), 2) Schema analysis workflow with component navigation, 3) Runtime schema discovery via Python API calls, 4) Source code analysis in invokeai/app/invocations/, 5) Workflow inspection of existing JSON files, 6) Frontend node template generation patterns. Included practical examples for SDXL model loader, jq commands for schema queries, and best practices for schema discovery and validation.
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754923706414_a8slzad1r" time="2025/08/11 22:48">
    <content>
      InvokeAI workflow schema discovery methods for Python client development:
    
      1. **OpenAPI Schema Endpoint**: curl http://localhost:9090/openapi.json provides complete node definitions, field constraints, validation rules, and default values
    
      2. **Key jq Commands for Schema Analysis**:
      - Find node schema: `jq --arg type &quot;NODE_TYPE&quot; &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == $type)) | .[0].value&#x27; openapi.json`
      - List input fields: `jq --arg type &quot;NODE_TYPE&quot; &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == $type)) | .[0].value.properties | to_entries[] | select(.value.field_kind == &quot;input&quot;) | .key&#x27; openapi.json`
      - Extract required fields: `jq &#x27;.components.schemas | to_entries | map(select(.value.properties.type.default == &quot;NODE_TYPE&quot;)) | .[0].value.properties | to_entries[] | select(.value.orig_required == true) | .key&#x27; openapi.json`
    
      3. **Common Field Types for Client Models**:
      - ImageField: `{&quot;image_name&quot;: &quot;uuid.png&quot;}`
      - LatentsField: `{&quot;latents_name&quot;: &quot;uuid&quot;}`
      - ModelField: `{&quot;key&quot;: &quot;model-id&quot;, &quot;hash&quot;: &quot;...&quot;, &quot;name&quot;: &quot;model-name&quot;, &quot;base&quot;: &quot;sdxl&quot;, &quot;type&quot;: &quot;main&quot;}`
      - Basic types: StringField, IntegerField, FloatField, BooleanField with validation constraints (ge, le, gt, lt)
    
      4. **Workflow Structure (v3.0.0)**: Essential components are nodes[] array with {id, type, data: {type, inputs, version, nodePack}}, edges[] array connecting nodes via sourceHandle/targetHandle, and meta.version for schema compatibility
    
      5. **Source Discovery**: Use `grep -r &quot;@invocation.*\&quot;NODE_TYPE\&quot;&quot; invokeai/app/invocations/` to find Python implementation files for understanding node behavior
    
      Reference: context/hints/invokeai-kb/howto-find-invokeai-workflow-schema.md
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1754925791966_k476kfg9q" time="2025/08/11 23:23">
    <content>
      Created comprehensive InvokeAI workflow input types documentation with 60+ field types identified from source code analysis:
    
      **Core Documentation**: `about-invokeai-workflow-input-types.md` covers:
      - 9 primitive types (int, float, bool, str, ColorField, BoundingBoxField, etc.)
      - 8 resource reference types (ImageField, LatentsField, TensorField, etc.)
      - 8 conditioning types for different model architectures (FLUX, SD3, CogView4)
      - 12+ model reference types (ModelIdentifierField, UNetField, CLIPField, etc.)
      - Collection types (list[T] for all supported types)
      - Enum/Literal types and UI configuration fields
      - Advanced features: connection types, validation constraints, mixins
    
      **Implementation Guide**: `howto-implement-invokeai-workflow-fields.md` provides:
      - Pydantic model patterns for client-side field validation
      - Node input builders with fluent API
      - Type-safe field validators and OpenAPI schema integration
      - Complete workflow construction examples (txt2img, img2img+ControlNet)
      - Connection graph utilities and dependency resolution
      - Error handling patterns and comprehensive test examples
    
      **Key Source Files Analyzed**:
      - `fields.py` - Field type definitions
      - `baseinvocation.py` - Base classes and decorators
      - `primitives.py` - Basic input/output types
      - `model.py` - Model reference types
      - 50+ invocation files - Usage patterns and validation rules
    
      Reference location: context/hints/invokeai-kb/
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755286020419_u5t9rz3n0" time="2025/08/16 03:27">
    <content>
      Created comprehensive InvokeAI workflow form field interpretation guide at context/hints/invokeai-kb/howto-interpret-workflow-form-field.md
    
      Key findings about workflow form structure:
      1. Form field creates hierarchical GUI with containers (row/column layout), node-fields (actual workflow inputs), dividers, and text elements
      2. Node-field elements reference workflow nodes via nodeId + fieldName
      3. Field labels come from node.data.inputs[field].label, fallback to node.data.label, then node.data.type
      4. Form structure is purely for UI, not workflow execution order
      5. exposedFields array should match node-field elements in form
    
      Critical JSONPath queries:
      - All form elements: $.form.elements.*
      - Node fields only: $.form.elements[?(@.type == &quot;node-field&quot;)]
      - Field references: $.form.elements[*].data.fieldIdentifier
      - Node label: $.nodes[?(@.id == &quot;nodeId&quot;)].data.inputs.fieldName.label
    
      Example: Positive Prompt field
      - Form element: node-field-I7XLCaNo7D
      - References: Node 0a167316-ba62-4218-9fcf-b3cff7963df8, field &quot;value&quot;
      - Field label from node: &quot;Positive Prompt&quot;
      - Display format: &quot;NodeLabel: FieldLabel&quot;
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755286376277_wft1tdu4p" time="2025/08/16 03:32">
    <content>
      CRITICAL: InvokeAI workflow form fields have total ordering with unique input-index
    
      The input-index is a 0-based index assigned to each node-field element based on depth-first tree traversal of the form structure. This index uniquely identifies each input field.
    
      Example from sdxl-flux-refine workflow (24 total input fields):
      - [0] Positive: Positive Prompt
      - [1] Negative: Negative Prompt
      - [2] integer: Output Width
      - [3] integer: Output Height
      - [4] sdxl_model_loader: SDXL Model
      ...
      - [23] float_math: Noise Ratio
    
      Key implementation details:
      1. Only node-field elements get an input-index (not containers, dividers, or text)
      2. Index is determined by tree traversal order starting from root
      3. Container children are processed in their listed order
      4. The input-index is essential for API calls where inputs may be provided as ordered arrays
    
      Python code to extract indices:
      ```python
      def traverse_form(elem_id, form_elements, input_fields):
      elem = form_elements.get(elem_id)
      if elem.get(&#x27;type&#x27;) == &#x27;container&#x27;:
      for child_id in elem[&#x27;data&#x27;].get(&#x27;children&#x27;, []):
      traverse_form(child_id, form_elements, input_fields)
      elif elem.get(&#x27;type&#x27;) == &#x27;node-field&#x27;:
      input_fields.append({&#x27;input_index&#x27;: len(input_fields), ...})
      ```
    
      Updated documentation at: context/hints/invokeai-kb/howto-interpret-workflow-form-field.md
    </content>
    <tags>#其他</tags>
  </item>
  <item id="mem_1755287246642_qc2hbthqh" time="2025/08/16 03:47">
    <content>
      InvokeAI Model Resolution for Workflows - CRITICAL
    
      Model references in workflows have deployment-specific keys and hashes. Only the model NAME is consistent across deployments.
    
      API Endpoint: GET /api/v2/models/get_by_attrs
      Parameters: name, type, base
    
      Example workflow model field:
      ```json
      {
      &quot;key&quot;: &quot;cc644930-283a-464f-b48f-831e6add8ed5&quot;,  // DEPLOYMENT-SPECIFIC
      &quot;hash&quot;: &quot;blake3:38aafa...&quot;,                     // MAY DIFFER
      &quot;name&quot;: &quot;t5_bnb_int8_quantized_encoder&quot;,        // CONSISTENT
      &quot;base&quot;: &quot;any&quot;,
      &quot;type&quot;: &quot;t5_encoder&quot;
      }
      ```
    
      Resolution process:
      1. Extract model name, type, base from workflow
      2. Query local InvokeAI instance: /api/v2/models/get_by_attrs
      3. Update workflow with local key and hash
      4. Submit workflow with correct local references
    
      Python implementation:
      ```python
      def find_model_by_name(base_url, model_name, model_type, model_base):
      response = requests.get(f&quot;{base_url}/api/v2/models/get_by_attrs&quot;,
      params={&quot;name&quot;: model_name, &quot;type&quot;: model_type, &quot;base&quot;: model_base})
      return response.json() if response.status_code == 200 else None
      ```
    
      Common model types:
      - t5_encoder: T5 text encoder for FLUX
      - main: Primary diffusion models (flux, sdxl)
      - vae: VAE models
      - clip: CLIP text encoders
    
      Task 1.2 completed in: context/tasks/features/task-explore-workflow.md
    </content>
    <tags>#其他</tags>
  </item>
</memory>